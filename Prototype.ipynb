{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60eb1ac0-51ea-4979-9807-38afc8833d3a",
   "metadata": {},
   "source": [
    "## 脳波分類"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edfe09e4-89fd-428f-a81d-dde7b1e34275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import Accuracy\n",
    "import hydra\n",
    "from omegaconf import DictConfig # Operate configs as a dict\n",
    "import wandb\n",
    "from termcolor import cprint\n",
    "from tqdm import tqdm\n",
    "from torcheeg import transforms\n",
    "\n",
    "from src.datasets import ThingsMEGDataset\n",
    "from src.models import BasicConvClassifier\n",
    "from src.utils import set_seed\n",
    "from src.preprocess import CAR, ToNDarray, extract_timepoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d960c71-fe98-4548-82fe-46b8a8fffc4c",
   "metadata": {},
   "source": [
    "### 設定の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "788ce503-103e-49bb-a419-fae256daea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "import yaml\n",
    "\n",
    "class AttrDict:\n",
    "    \"\"\"\n",
    "    辞書を受け取り、属性アクセス可能なオブジェクトに変換するクラスです。\n",
    "    \"\"\"\n",
    "    def __init__(self, dictionary: dict):\n",
    "        for key, value in dictionary.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "with open('configs\\config.yaml') as file:\n",
    "    args = yaml.safe_load(file.read())\n",
    "\n",
    "args = AttrDict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c268319-8fcc-4b38-aae3-e490b3269e09",
   "metadata": {},
   "source": [
    "### データの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5d3519-e1ed-41ca-9537-cd4c4cfdd647",
   "metadata": {},
   "source": [
    "#### 自作のデータセットの作成（dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f99c01d-59c3-4e1c-9ddc-cdd8caadfe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Tuple\n",
    "from termcolor import cprint\n",
    "\n",
    "\n",
    "class ThingsMEGDataset(torch.utils.data.Dataset):\n",
    "    # Train = ThingsMEGDataset(\"train\",data_dir)\n",
    "    # \n",
    "    # Methods:\n",
    "    # Train.split: data type\n",
    "    # Train.num_classes: number of classes\n",
    "    # Train.X: data [n, ch, seq]\n",
    "    # Train.subject_idxs: subject index for each sample\n",
    "    # Train.y: true labels\n",
    "    \n",
    "    def __init__(self, split: str, data_dir: str = \"data\", transform = None) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        assert split in [\"train\", \"val\", \"test\"], f\"Invalid split: {split}\"\n",
    "        self.split = split\n",
    "        self.num_classes = 1854\n",
    "        \n",
    "        self.X = torch.load(os.path.join(data_dir, f\"{split}_X.pt\"))\n",
    "        self.subject_idxs = torch.load(os.path.join(data_dir, f\"{split}_subject_idxs.pt\"))\n",
    "        self.n_subject = len(self.subject_idxs.unique())\n",
    "        \n",
    "        self.transform = transform\n",
    "        \n",
    "        if split in [\"train\", \"val\"]:\n",
    "            self.y = torch.load(os.path.join(data_dir, f\"{split}_y.pt\"))\n",
    "            assert len(torch.unique(self.y)) == self.num_classes, \"Number of classes do not match.\"\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.X[i]\n",
    "        if self.transform:\n",
    "            x = self.transform(eeg=x)['eeg']\n",
    "            \n",
    "        if hasattr(self, \"y\"):\n",
    "            return x, self.y[i], self.subject_idxs[i]\n",
    "        else:\n",
    "            return self.X[i], self.subject_idxs[i]\n",
    "        \n",
    "    @property\n",
    "    def num_channels(self) -> int:\n",
    "        return self.X.shape[1]\n",
    "    \n",
    "    @property\n",
    "    def seq_len(self) -> int:\n",
    "        return self.X.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc656142-97fc-463c-8ecd-2c0313d8cb81",
   "metadata": {},
   "source": [
    "#### Transformクラスの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0670cca0-ae27-42a6-8fee-7400a95bfe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToNDarray(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, eeg):\n",
    "        x = eeg\n",
    "        x = x.detach().clone().cpu()   #x=(C,N,T)\n",
    "        x = x.numpy()   #x=(C,N,T)\n",
    "        \n",
    "        return {'eeg': x}\n",
    "\n",
    "class CAR(object): # Global contrast normalization\n",
    "    '''\n",
    "    Class to process common median reference for eeg signals. Input data should be a tensor with a shape [C, T].\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, eeg):\n",
    "        x = eeg\n",
    "        # noises inside channels\n",
    "        inner_med = x.median(dim=-1).values\n",
    "        x = x.t()\n",
    "        x -= inner_med\n",
    "        x = x.t()\n",
    "        \n",
    "        # noises shared among channels\n",
    "        inter_med = x.median(dim=-2).values \n",
    "        x -= inter_med\n",
    "\n",
    "        return {'eeg': x}\n",
    "\n",
    "class extract_timepoint(object):\n",
    "    def __init__(self, start, end):\n",
    "        # time window to extract\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "\n",
    "    def __call__(self, eeg):\n",
    "        x = eeg\n",
    "        x = x[:,self.start:self.end]\n",
    "        return {'eeg': x}\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c217c-eb74-4ddb-8cd9-97a919131c48",
   "metadata": {},
   "source": [
    "#### データローダー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d1b78442-a5b8-4971-ae67-a4d1046c27a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1084, 281)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = transforms.BandSignal(sampling_rate=200)\n",
    "X = train_set[0][0]\n",
    "c(eeg=X)['eeg'].reshape((-1,281)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8e139bcf-3f18-4987-993a-ab8289f4d227",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(args.seed)\n",
    "\n",
    "# ------------------\n",
    "#    Dataloader\n",
    "# ------------------\n",
    "# loader_args = {\"batch_size\": args.batch_size, \"num_workers\": args.num_workers}\n",
    "loader_args = {\"batch_size\": args.batch_size, \"num_workers\": 1}\n",
    "train_set = ThingsMEGDataset(\"train\", args.data_dir) # [n, ch, seq]\n",
    "train_set.transform = transforms.Compose([\n",
    "    # CAR(),\n",
    "    ToNDarray(),\n",
    "    transforms.MeanStdNormalize(axis=-1),\n",
    "    transforms.ToTensor()\n",
    "    # RandomMask()\n",
    "])\n",
    "train_loader = torch.utils.data.DataLoader(train_set, shuffle=True, **loader_args)\n",
    "\n",
    "# Load valid data\n",
    "val_set = ThingsMEGDataset(\"val\", args.data_dir) # [n, ch, seq]\n",
    "val_set.transform = transforms.Compose([\n",
    "    # CAR(),\n",
    "    ToNDarray(),\n",
    "    transforms.MeanStdNormalize(axis=-1),\n",
    "    transforms.ToTensor()\n",
    "    # RandomMask()\n",
    "])\n",
    "val_loader = torch.utils.data.DataLoader(val_set, shuffle=False, **loader_args)\n",
    "\n",
    "# Load test data\n",
    "test_set = ThingsMEGDataset(\"test\", args.data_dir) # [n, ch, seq]\n",
    "test_set.transform = transforms.Compose([\n",
    "    # CAR(),\n",
    "    ToNDarray(),\n",
    "    transforms.MeanStdNormalize(axis=-1),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_set, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c982223d-e8d5-4506-bb63-7c3da1c442b7",
   "metadata": {},
   "source": [
    "### モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2d0969c-1520-4aa8-9634-489c0705b3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Original models\n",
    "class BasicConvClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        in_channels: int,\n",
    "        hid_dim: int = 128\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "            ConvBlock(hid_dim, hid_dim),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b d 1 -> b d\"),\n",
    "            nn.Linear(hid_dim, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X ( b, c, t ): _description_\n",
    "        Returns:\n",
    "            X ( b, num_classes ): _description_\n",
    "        \"\"\"\n",
    "        X = self.blocks(X)\n",
    "\n",
    "        return self.head(X)\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        kernel_size: int = 3,\n",
    "        p_drop: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.conv1 = nn.Conv1d(out_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        # self.conv2 = nn.Conv1d(out_dim, out_dim, kernel_size) # , padding=\"same\")\n",
    "        \n",
    "        self.batchnorm0 = nn.BatchNorm1d(num_features=out_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=out_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        if self.in_dim == self.out_dim:\n",
    "            X = self.conv0(X) + X  # skip connection\n",
    "        else:\n",
    "            X = self.conv0(X)\n",
    "\n",
    "        X = F.gelu(self.batchnorm0(X))\n",
    "\n",
    "        X = self.conv1(X) + X  # skip connection\n",
    "        X = F.gelu(self.batchnorm1(X))\n",
    "\n",
    "        # X = self.conv2(X)\n",
    "        # X = F.glu(X, dim=-2)\n",
    "\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "68fa9cf7-8acf-4b7b-9096-f2e564de5bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from einops.layers.torch import Rearrange\n",
    "\n",
    "# Model based on Defossez et al., 2023\n",
    "# 1. Convolution layer without activation\n",
    "# 2. Subject dependent convlution layer without activation\n",
    "# 3. \n",
    "class DefossezClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int,\n",
    "        seq_len: int,\n",
    "        in_channels: int,\n",
    "        hid_dim: int = 320,\n",
    "        n_subject: int = 4,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            SimpleConvBlock(in_dim=in_channels, out_dim=in_channels, kernel_size=1),\n",
    "            SubjectBlock(in_channels,in_channels,n_subject),\n",
    "            ConvBlock(in_dim=in_channels,hid_dim=hid_dim,out_dim=hid_dim*2,kernel_size=3,k=0),\n",
    "            ConvBlock(in_dim=hid_dim,hid_dim=hid_dim,out_dim=hid_dim*2,kernel_size=3,k=1),\n",
    "            ConvBlock(in_dim=hid_dim,hid_dim=hid_dim,out_dim=hid_dim*2,kernel_size=3,k=2),\n",
    "            ConvBlock(in_dim=hid_dim,hid_dim=hid_dim,out_dim=hid_dim*2,kernel_size=3,k=3),\n",
    "            ConvBlock(in_dim=hid_dim,hid_dim=hid_dim,out_dim=hid_dim*2,kernel_size=3,k=4),\n",
    "            SimpleConvBlock(in_dim = hid_dim, out_dim = hid_dim*2, kernel_size=1, activate = True),\n",
    "            SimpleConvBlock(in_dim = hid_dim*2, out_dim = hid_dim*2, kernel_size=1, activate = True),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b d 1 -> b d\"),\n",
    "            nn.Linear(hid_dim*2, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor, subject_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"_summary_\n",
    "        Args:\n",
    "            X ( b, c, t ): _description_\n",
    "        Returns:\n",
    "            X ( b, num_classes ): _description_\n",
    "        \"\"\"\n",
    "        X = self.blocks(X, subject_idxs)\n",
    "\n",
    "        return self.head(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "648bd6c5-4646-4487-aa40-ac7e2db68f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim,\n",
    "        out_dim,\n",
    "        kernel_size: int = 1,\n",
    "        p_drop: float = 0.1,\n",
    "        activate: bool = False,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.activate = activate\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, out_dim, kernel_size, padding=\"same\")\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, subject_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        X = self.conv0(X)\n",
    "        if self.activate:\n",
    "            X = F.gelu(X)\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d70530f2-7b44-4577-ae83-16d86ff4b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim = 271,\n",
    "        hid_dim = 320,\n",
    "        out_dim = 640,\n",
    "        kernel_size: int = 3,\n",
    "        k: int = 0,\n",
    "        p_drop: float = 0.1,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "\n",
    "        self.conv0 = nn.Conv1d(in_dim, hid_dim, kernel_size, padding=\"same\",dilation=2**(2*k)%5)\n",
    "        self.conv1 = nn.Conv1d(hid_dim, hid_dim, kernel_size, padding=\"same\",dilation=2**(2*k+1)%5)\n",
    "        self.conv2 = nn.Conv1d(hid_dim, out_dim, kernel_size, padding=\"same\",dilation=2)\n",
    "        \n",
    "        self.batchnorm0 = nn.BatchNorm1d(num_features=hid_dim)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(num_features=hid_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(p_drop)\n",
    "\n",
    "    def forward(self, X: torch.Tensor, subject_idxs: torch.Tensor) -> torch.Tensor:\n",
    "        if self.in_dim == self.hid_dim:\n",
    "            X = self.conv0(X) + X  # skip connection\n",
    "        else:\n",
    "            X = self.conv0(X)\n",
    "        X = F.gelu(self.batchnorm0(X))\n",
    "\n",
    "        X = self.conv1(X) + X  # skip connection\n",
    "        X = F.gelu(self.batchnorm1(X))\n",
    "        \n",
    "        X = self.conv2(X)\n",
    "        X = F.glu(X, dim=-2) # No normalization for 2nd convolution\n",
    "\n",
    "        return self.dropout(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f1c9236f-a31c-45d0-adcc-fff4302ecd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubjectBlock(nn.Module):\n",
    "    \"\"\"Subject linear layer\"\"\"\n",
    "    def __init__(self, in_channels: int, out_channels: int, n_subjects: int = 4):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(n_subjects, in_channels, out_channels)) # [S,C_in,C_out]\n",
    "        self.weights.data *= 1 / in_channels**0.5 # Xavier initialization\n",
    "\n",
    "    def forward(self, X:torch.Tensor, subject_idxs: torch.Tensor):\n",
    "        _, C_in, C_out = self.weights.shape\n",
    "        weights = self.weights.gather(0, subject_idxs.view(-1, 1, 1).expand(-1, C_in, C_out)) # Assign subject-specific weights: [B,C_in,C_out]\n",
    "        return torch.einsum(\"bct,bcd->bdt\", X, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9c2fbf8-fc2d-4434-ab86-ab3792670277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([128, 271, 281])\n",
      "Subject index: torch.Size([128])\n",
      "1st conv block: torch.Size([128, 271, 281])\n",
      "Subject block: torch.Size([128, 271, 281])\n",
      "Conv block (k=0): torch.Size([128, 320, 281])\n",
      "Conv block (k=1): torch.Size([128, 320, 281])\n",
      "Conv block (k=2): torch.Size([128, 320, 281])\n",
      "Conv block (k=3): torch.Size([128, 320, 281])\n",
      "Conv block (k=4): torch.Size([128, 320, 281])\n",
      "2bd conv block: torch.Size([128, 640, 281])\n",
      "Final conv block: torch.Size([128, 640, 281])\n",
      "After adaptive pooling: torch.Size([128, 640, 1])\n",
      "After rearrangement: torch.Size([128, 640])\n",
      "Final output: torch.Size([128, 1854])\n"
     ]
    }
   ],
   "source": [
    "X = train_set[0:128][0]\n",
    "subject_idxs = train_set[0:128][2]\n",
    "n_subject = train_set.n_subject\n",
    "print(f\"Original shape: {X.shape}\") # [B,C,T]\n",
    "print(f\"Subject index: {subject_idxs.shape}\") # [B]\n",
    "\n",
    "c = SimpleConvBlock(271, 271, kernel_size=1)\n",
    "X = c(X) # [B,C_in,T] -> [B,C_out,T]\n",
    "print(f\"1st conv block: {X.shape}\")\n",
    "\n",
    "c = SubjectLayers(271,271,n_subject)\n",
    "X = c(X,subject_idxs) # [B,C_in,T] -> [B,C_out,T]\n",
    "print(f\"Subject block: {X.shape}\") # [B,C,T]\n",
    "\n",
    "c = ConvBlock(in_dim=271,hid_dim=320,out_dim=640,kernel_size=3,k=0)\n",
    "X = c(X)\n",
    "print(f\"Conv block (k=0): {X.shape}\") # [B,C,T]\n",
    "\n",
    "c = ConvBlock(in_dim=320,hid_dim=320,out_dim=640,kernel_size=3,k=1)\n",
    "X = c(X)\n",
    "print(f\"Conv block (k=1): {X.shape}\") # [B,C,T]\n",
    "\n",
    "c = ConvBlock(in_dim=320,hid_dim=320,out_dim=640,kernel_size=3,k=2)\n",
    "X = c(X)\n",
    "print(f\"Conv block (k=2): {X.shape}\") # [B,C,T]\n",
    "\n",
    "c = ConvBlock(in_dim=320,hid_dim=320,out_dim=640,kernel_size=3,k=3)\n",
    "X = c(X)\n",
    "print(f\"Conv block (k=3): {X.shape}\") # [B,C,T]\n",
    "\n",
    "c = ConvBlock(in_dim=320,hid_dim=320,out_dim=640,kernel_size=3,k=4)\n",
    "X = c(X)\n",
    "print(f\"Conv block (k=4): {X.shape}\") # [B,C,T]\n",
    "\n",
    "c = SimpleConvBlock(in_dim = 320, out_dim = 640, kernel_size=1, activate = True)\n",
    "X = c(X)\n",
    "print(f\"2bd conv block: {X.shape}\")\n",
    "\n",
    "c = SimpleConvBlock(in_dim = 640, out_dim = 640, kernel_size=1, activate = True)\n",
    "X = c(X)\n",
    "print(f\"Final conv block: {X.shape}\")\n",
    "\n",
    "c = nn.AdaptiveAvgPool1d(1)\n",
    "X = c(X)\n",
    "print(f\"After adaptive pooling: {X.shape}\")\n",
    "c = Rearrange(\"b d 1 -> b d\")\n",
    "X = c(X)\n",
    "print(f\"After rearrangement: {X.shape}\")\n",
    "c = nn.Linear(640, 1854)\n",
    "X = c(X)\n",
    "print(f\"Final output: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e44a0675-6c1f-475d-ba8c-13587b594da7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1854])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train_set[0:128][0]\n",
    "model = BasicConvClassifier(\n",
    "    train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    ")\n",
    "model.forward(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971ce6b-f550-45a5-a72b-bb5887a02896",
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.blocks = nn.Sequential(\n",
    "            ConvBlock(in_channels, hid_dim),\n",
    "            ConvBlock(hid_dim, hid_dim),\n",
    "        )\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            Rearrange(\"b d 1 -> b d\"),\n",
    "            nn.Linear(hid_dim, num_classes),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251509a8-f439-4c57-a01f-e52ffb54c66a",
   "metadata": {},
   "source": [
    "### モデルの訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1a8a4c63-8741-43bd-9fa1-341049537b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "#       Model\n",
    "# ------------------\n",
    "model = BasicConvClassifier(\n",
    "    train_set.num_classes, train_set.seq_len, train_set.num_channels\n",
    ").to(args.device)\n",
    "\n",
    "# ------------------\n",
    "#     Optimizer\n",
    "# ------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64da579-2f63-4666-951a-91577620ecde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ------------------\n",
    "#   Start training\n",
    "# ------------------  \n",
    "max_val_acc = 0\n",
    "accuracy = Accuracy(\n",
    "    task=\"multiclass\", num_classes=train_set.num_classes, top_k=10\n",
    ").to(args.device)\n",
    "  \n",
    "for epoch in range(args.epochs):\n",
    "    print(f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "    \n",
    "    train_loss, train_acc, val_loss, val_acc = [], [], [], []\n",
    "    \n",
    "    model.train()\n",
    "    for X, y, subject_idxs in tqdm(train_loader, desc=\"Train\"):\n",
    "        X, y = X.to(args.device), y.to(args.device)\n",
    "\n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = F.cross_entropy(y_pred, y)\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        acc = accuracy(y_pred, y)\n",
    "        train_acc.append(acc.item())\n",
    "\n",
    "    model.eval()\n",
    "    for X, y, subject_idxs in tqdm(val_loader, desc=\"Validation\"):\n",
    "        X, y = X.to(args.device), y.to(args.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            y_pred = model(X)\n",
    "        \n",
    "        val_loss.append(F.cross_entropy(y_pred, y).item())\n",
    "        val_acc.append(accuracy(y_pred, y).item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{args.epochs} | train loss: {np.mean(train_loss):.3f} | train acc: {np.mean(train_acc):.3f} | val loss: {np.mean(val_loss):.3f} | val acc: {np.mean(val_acc):.3f}\")\n",
    "    torch.save(model.state_dict(), os.path.join(logdir, \"model_last.pt\"))\n",
    "    if args.use_wandb:\n",
    "        wandb.log({\"train_loss\": np.mean(train_loss), \"train_acc\": np.mean(train_acc), \"val_loss\": np.mean(val_loss), \"val_acc\": np.mean(val_acc)})\n",
    "    \n",
    "    if np.mean(val_acc) > max_val_acc:\n",
    "        cprint(\"New best.\", \"cyan\")\n",
    "        torch.save(model.state_dict(), os.path.join(logdir, \"model_best.pt\"))\n",
    "        max_val_acc = np.mean(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbf27c0-c336-4ff8-a41f-795d77c70d61",
   "metadata": {},
   "source": [
    "### モデルの検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ba235d93-005a-45c7-ab0b-0abe91a0fd39",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for BasicConvClassifier:\n\tMissing key(s) in state_dict: \"blocks.0.conv0.weight\", \"blocks.0.conv0.bias\", \"blocks.0.conv1.weight\", \"blocks.0.conv1.bias\", \"blocks.0.batchnorm0.weight\", \"blocks.0.batchnorm0.bias\", \"blocks.0.batchnorm0.running_mean\", \"blocks.0.batchnorm0.running_var\", \"blocks.0.batchnorm1.weight\", \"blocks.0.batchnorm1.bias\", \"blocks.0.batchnorm1.running_mean\", \"blocks.0.batchnorm1.running_var\", \"blocks.1.conv0.weight\", \"blocks.1.conv0.bias\", \"blocks.1.conv1.weight\", \"blocks.1.conv1.bias\", \"blocks.1.batchnorm0.weight\", \"blocks.1.batchnorm0.bias\", \"blocks.1.batchnorm0.running_mean\", \"blocks.1.batchnorm0.running_var\", \"blocks.1.batchnorm1.weight\", \"blocks.1.batchnorm1.bias\", \"blocks.1.batchnorm1.running_mean\", \"blocks.1.batchnorm1.running_var\". \n\tUnexpected key(s) in state_dict: \"pre_conv_block.0.conv0.weight\", \"pre_conv_block.0.conv0.bias\", \"subject_block.weights\", \"post_conv_block.0.conv0.weight\", \"post_conv_block.0.conv0.bias\", \"post_conv_block.0.conv1.weight\", \"post_conv_block.0.conv1.bias\", \"post_conv_block.0.conv2.weight\", \"post_conv_block.0.conv2.bias\", \"post_conv_block.0.batchnorm0.weight\", \"post_conv_block.0.batchnorm0.bias\", \"post_conv_block.0.batchnorm0.running_mean\", \"post_conv_block.0.batchnorm0.running_var\", \"post_conv_block.0.batchnorm0.num_batches_tracked\", \"post_conv_block.0.batchnorm1.weight\", \"post_conv_block.0.batchnorm1.bias\", \"post_conv_block.0.batchnorm1.running_mean\", \"post_conv_block.0.batchnorm1.running_var\", \"post_conv_block.0.batchnorm1.num_batches_tracked\", \"post_conv_block.1.conv0.weight\", \"post_conv_block.1.conv0.bias\", \"post_conv_block.1.conv1.weight\", \"post_conv_block.1.conv1.bias\", \"post_conv_block.1.conv2.weight\", \"post_conv_block.1.conv2.bias\", \"post_conv_block.1.batchnorm0.weight\", \"post_conv_block.1.batchnorm0.bias\", \"post_conv_block.1.batchnorm0.running_mean\", \"post_conv_block.1.batchnorm0.running_var\", \"post_conv_block.1.batchnorm0.num_batches_tracked\", \"post_conv_block.1.batchnorm1.weight\", \"post_conv_block.1.batchnorm1.bias\", \"post_conv_block.1.batchnorm1.running_mean\", \"post_conv_block.1.batchnorm1.running_var\", \"post_conv_block.1.batchnorm1.num_batches_tracked\", \"post_conv_block.2.conv0.weight\", \"post_conv_block.2.conv0.bias\", \"post_conv_block.2.conv1.weight\", \"post_conv_block.2.conv1.bias\", \"post_conv_block.2.conv2.weight\", \"post_conv_block.2.conv2.bias\", \"post_conv_block.2.batchnorm0.weight\", \"post_conv_block.2.batchnorm0.bias\", \"post_conv_block.2.batchnorm0.running_mean\", \"post_conv_block.2.batchnorm0.running_var\", \"post_conv_block.2.batchnorm0.num_batches_tracked\", \"post_conv_block.2.batchnorm1.weight\", \"post_conv_block.2.batchnorm1.bias\", \"post_conv_block.2.batchnorm1.running_mean\", \"post_conv_block.2.batchnorm1.running_var\", \"post_conv_block.2.batchnorm1.num_batches_tracked\", \"post_conv_block.3.conv0.weight\", \"post_conv_block.3.conv0.bias\", \"post_conv_block.3.conv1.weight\", \"post_conv_block.3.conv1.bias\", \"post_conv_block.3.conv2.weight\", \"post_conv_block.3.conv2.bias\", \"post_conv_block.3.batchnorm0.weight\", \"post_conv_block.3.batchnorm0.bias\", \"post_conv_block.3.batchnorm0.running_mean\", \"post_conv_block.3.batchnorm0.running_var\", \"post_conv_block.3.batchnorm0.num_batches_tracked\", \"post_conv_block.3.batchnorm1.weight\", \"post_conv_block.3.batchnorm1.bias\", \"post_conv_block.3.batchnorm1.running_mean\", \"post_conv_block.3.batchnorm1.running_var\", \"post_conv_block.3.batchnorm1.num_batches_tracked\", \"post_conv_block.4.conv0.weight\", \"post_conv_block.4.conv0.bias\", \"post_conv_block.4.conv1.weight\", \"post_conv_block.4.conv1.bias\", \"post_conv_block.4.conv2.weight\", \"post_conv_block.4.conv2.bias\", \"post_conv_block.4.batchnorm0.weight\", \"post_conv_block.4.batchnorm0.bias\", \"post_conv_block.4.batchnorm0.running_mean\", \"post_conv_block.4.batchnorm0.running_var\", \"post_conv_block.4.batchnorm0.num_batches_tracked\", \"post_conv_block.4.batchnorm1.weight\", \"post_conv_block.4.batchnorm1.bias\", \"post_conv_block.4.batchnorm1.running_mean\", \"post_conv_block.4.batchnorm1.running_var\", \"post_conv_block.4.batchnorm1.num_batches_tracked\", \"post_conv_block.5.conv0.weight\", \"post_conv_block.5.conv0.bias\", \"post_conv_block.6.conv0.weight\", \"post_conv_block.6.conv0.bias\". \n\tsize mismatch for head.2.weight: copying a param with shape torch.Size([1854, 640]) from checkpoint, the shape in current model is torch.Size([1854, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#  Start evaluation with best model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ----------------------------------\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m2024-06-19\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m19-07-08\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mmodel_best.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m preds \u001b[38;5;241m=\u001b[39m [] \n\u001b[0;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dlbasics\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BasicConvClassifier:\n\tMissing key(s) in state_dict: \"blocks.0.conv0.weight\", \"blocks.0.conv0.bias\", \"blocks.0.conv1.weight\", \"blocks.0.conv1.bias\", \"blocks.0.batchnorm0.weight\", \"blocks.0.batchnorm0.bias\", \"blocks.0.batchnorm0.running_mean\", \"blocks.0.batchnorm0.running_var\", \"blocks.0.batchnorm1.weight\", \"blocks.0.batchnorm1.bias\", \"blocks.0.batchnorm1.running_mean\", \"blocks.0.batchnorm1.running_var\", \"blocks.1.conv0.weight\", \"blocks.1.conv0.bias\", \"blocks.1.conv1.weight\", \"blocks.1.conv1.bias\", \"blocks.1.batchnorm0.weight\", \"blocks.1.batchnorm0.bias\", \"blocks.1.batchnorm0.running_mean\", \"blocks.1.batchnorm0.running_var\", \"blocks.1.batchnorm1.weight\", \"blocks.1.batchnorm1.bias\", \"blocks.1.batchnorm1.running_mean\", \"blocks.1.batchnorm1.running_var\". \n\tUnexpected key(s) in state_dict: \"pre_conv_block.0.conv0.weight\", \"pre_conv_block.0.conv0.bias\", \"subject_block.weights\", \"post_conv_block.0.conv0.weight\", \"post_conv_block.0.conv0.bias\", \"post_conv_block.0.conv1.weight\", \"post_conv_block.0.conv1.bias\", \"post_conv_block.0.conv2.weight\", \"post_conv_block.0.conv2.bias\", \"post_conv_block.0.batchnorm0.weight\", \"post_conv_block.0.batchnorm0.bias\", \"post_conv_block.0.batchnorm0.running_mean\", \"post_conv_block.0.batchnorm0.running_var\", \"post_conv_block.0.batchnorm0.num_batches_tracked\", \"post_conv_block.0.batchnorm1.weight\", \"post_conv_block.0.batchnorm1.bias\", \"post_conv_block.0.batchnorm1.running_mean\", \"post_conv_block.0.batchnorm1.running_var\", \"post_conv_block.0.batchnorm1.num_batches_tracked\", \"post_conv_block.1.conv0.weight\", \"post_conv_block.1.conv0.bias\", \"post_conv_block.1.conv1.weight\", \"post_conv_block.1.conv1.bias\", \"post_conv_block.1.conv2.weight\", \"post_conv_block.1.conv2.bias\", \"post_conv_block.1.batchnorm0.weight\", \"post_conv_block.1.batchnorm0.bias\", \"post_conv_block.1.batchnorm0.running_mean\", \"post_conv_block.1.batchnorm0.running_var\", \"post_conv_block.1.batchnorm0.num_batches_tracked\", \"post_conv_block.1.batchnorm1.weight\", \"post_conv_block.1.batchnorm1.bias\", \"post_conv_block.1.batchnorm1.running_mean\", \"post_conv_block.1.batchnorm1.running_var\", \"post_conv_block.1.batchnorm1.num_batches_tracked\", \"post_conv_block.2.conv0.weight\", \"post_conv_block.2.conv0.bias\", \"post_conv_block.2.conv1.weight\", \"post_conv_block.2.conv1.bias\", \"post_conv_block.2.conv2.weight\", \"post_conv_block.2.conv2.bias\", \"post_conv_block.2.batchnorm0.weight\", \"post_conv_block.2.batchnorm0.bias\", \"post_conv_block.2.batchnorm0.running_mean\", \"post_conv_block.2.batchnorm0.running_var\", \"post_conv_block.2.batchnorm0.num_batches_tracked\", \"post_conv_block.2.batchnorm1.weight\", \"post_conv_block.2.batchnorm1.bias\", \"post_conv_block.2.batchnorm1.running_mean\", \"post_conv_block.2.batchnorm1.running_var\", \"post_conv_block.2.batchnorm1.num_batches_tracked\", \"post_conv_block.3.conv0.weight\", \"post_conv_block.3.conv0.bias\", \"post_conv_block.3.conv1.weight\", \"post_conv_block.3.conv1.bias\", \"post_conv_block.3.conv2.weight\", \"post_conv_block.3.conv2.bias\", \"post_conv_block.3.batchnorm0.weight\", \"post_conv_block.3.batchnorm0.bias\", \"post_conv_block.3.batchnorm0.running_mean\", \"post_conv_block.3.batchnorm0.running_var\", \"post_conv_block.3.batchnorm0.num_batches_tracked\", \"post_conv_block.3.batchnorm1.weight\", \"post_conv_block.3.batchnorm1.bias\", \"post_conv_block.3.batchnorm1.running_mean\", \"post_conv_block.3.batchnorm1.running_var\", \"post_conv_block.3.batchnorm1.num_batches_tracked\", \"post_conv_block.4.conv0.weight\", \"post_conv_block.4.conv0.bias\", \"post_conv_block.4.conv1.weight\", \"post_conv_block.4.conv1.bias\", \"post_conv_block.4.conv2.weight\", \"post_conv_block.4.conv2.bias\", \"post_conv_block.4.batchnorm0.weight\", \"post_conv_block.4.batchnorm0.bias\", \"post_conv_block.4.batchnorm0.running_mean\", \"post_conv_block.4.batchnorm0.running_var\", \"post_conv_block.4.batchnorm0.num_batches_tracked\", \"post_conv_block.4.batchnorm1.weight\", \"post_conv_block.4.batchnorm1.bias\", \"post_conv_block.4.batchnorm1.running_mean\", \"post_conv_block.4.batchnorm1.running_var\", \"post_conv_block.4.batchnorm1.num_batches_tracked\", \"post_conv_block.5.conv0.weight\", \"post_conv_block.5.conv0.bias\", \"post_conv_block.6.conv0.weight\", \"post_conv_block.6.conv0.bias\". \n\tsize mismatch for head.2.weight: copying a param with shape torch.Size([1854, 640]) from checkpoint, the shape in current model is torch.Size([1854, 128])."
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "#  Start evaluation with best model\n",
    "# ----------------------------------\n",
    "model.load_state_dict(torch.load(\"outputs\\\\2024-06-19\\\\19-07-08\\\\model_best.pt\", map_location=args.device))\n",
    "\n",
    "preds = [] \n",
    "model.eval()\n",
    "for X, subject_idxs in tqdm(test_loader, desc=\"Validation\"):        \n",
    "    preds.append(model(X.to(args.device)).detach().cpu())\n",
    "    \n",
    "preds = torch.cat(preds, dim=0).numpy()\n",
    "np.save(os.path.join(logdir, \"submission\"), preds)\n",
    "cprint(f\"Submission {preds.shape} saved at {logdir}\", \"cyan\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
